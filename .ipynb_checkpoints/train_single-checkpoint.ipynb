{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from helpful_files.networks import Network\n",
    "from helpful_files.training import *\n",
    "\n",
    "from KiTS_dataset import KiTS_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "# General settings\n",
    "datapath = './'                     # The location of your train, test, repr, and query folders. Make sure it ends in '/'!\n",
    "savepath = 'myModel.pth'            # Where should your trained model(s) be saved, and under what name?\n",
    "gpu = 0                             # What gpu do you wish to train on?\n",
    "workers = 6                         # Number of cpu worker processes to use for data loading\n",
    "epoch = 10                          # Number of passes over the dataset before the learning rate is cut\n",
    "ncuts = 5                           # Number of times to cut the learning rate before training completes\n",
    "verbosity = 5                       # How many batches in between status updates \n",
    "ensemble = 3                        # How many models to train in parallel\n",
    "torch.cuda.set_device(gpu)\n",
    "cudnn.benchmark = True\n",
    "trn_pct = 0.9                       #test/val split pct\n",
    "\n",
    "# Batch construction\n",
    "way = 4                            # Number of classes per batch during training\n",
    "trainshot = 2                       # Number of images per class used to form prototypes\n",
    "testshot = 2                       # Number of images per class used to make predictions\n",
    "\n",
    "# Model construction\n",
    "folding = True                      # Use batch folding?\n",
    "covariance_pooling = True           # Use covariance pooling?\n",
    "localizing = True                   # Use localization?\n",
    "fewshot_local = False                # If you are using localization: few-shot, or parametric? Few-shot if True, param if False\n",
    "network_width = 32                  # Number of channels at every layer of the network\n",
    "\n",
    "# Data loading\n",
    "augmentation_flipping = True        # Horizontal flip data augmentation\n",
    "include_masks = (localizing         # Include or ignore the bounding box annotations?\n",
    "                 and fewshot_local)\n",
    "data_shape = (128,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data \n",
    "#this step takes forever \n",
    "#sampler has to iterate through whole dataset and dataset is real big\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "d_boxes = torch.load('helpful_files/box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\"\"\"\n",
    "if folding:\n",
    "    # Batch folding has no reference/query distinction\n",
    "    shots = [trainshot+testshot]\n",
    "else:\n",
    "    # Standard setup\n",
    "    shots = [trainshot, testshot]\n",
    "if localizing and fewshot_local and not folding:\n",
    "    # Unfolded prototype localizers need another set of reference images to inform foreground/background predictions\n",
    "    shots = [trainshot, trainshot, testshot-trainshot]\n",
    "    \n",
    "\"\"\"\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    datapath+'train', \n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, augmentation_flipping, include_masks))\n",
    "\"\"\"\n",
    "trainset = KiTS_Set(data_shape)\n",
    "\n",
    "#train_data,val_data = torch.utils.data.random_split(trainset,[round(trn_pct*len(trainset)), round((1-trn_pct)*len(trainset))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_sampler = ProtoSampler(train_data, way, shots),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "\"\"\"\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data, \n",
    "    batch_sampler = ProtoSampler(val_data, way, shots),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "\"\"\"\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Models\n",
    "    \n",
    "models = [Network(network_width, folding, covariance_pooling, \n",
    "                  localizing, fewshot_local, shots).cuda() \n",
    "          for i in range(ensemble)]\n",
    "optimizer = [optim.Adam(m.parameters(), lr=.0001) for m in models]\n",
    "scheduler = [optim.lr_scheduler.LambdaLR(o, lambda x: 1/(2**x)) for o in optimizer]\n",
    "criterion = NLLLoss().cuda()\n",
    "\n",
    "expander = avgpool()\n",
    "if localizing:\n",
    "    if fewshot_local:\n",
    "        expander = fsCL if covariance_pooling else fsL\n",
    "    else:\n",
    "        expander = pCL() if covariance_pooling else pL()\n",
    "elif covariance_pooling:\n",
    "    expander = covapool\n",
    "expanders = [expander for _ in range(ensemble)]\n",
    "\n",
    "if localizing and not fewshot_local:\n",
    "    fbcentroids = torch.load(model[:model.rfind('.')]+'_localizers'+model[model.rfind('.'):])\n",
    "    for i in range(ensemble):\n",
    "        expanders[i].centroids.data = fbcentroids[i]\n",
    "        expanders[i].cuda()\n",
    "\n",
    "\n",
    "nweights = sum([i.numel() for i in list(models[0].parameters())])\n",
    "print(nweights,\"parameters in each neural net.\")\n",
    "print('Ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the Thing!\n",
    "\n",
    "start = time.time()\n",
    "trainlosses, acctracker = [[] for _ in range(ensemble)],[[] for _ in range(ensemble)]\n",
    "epochs = ncuts*epoch\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    trainloss, acc = train(train_loader, models, optimizer, criterion, way, shots, verbosity)\n",
    "    \n",
    "    # Update the graphics, report\n",
    "    display.clear_output(wait=True)\n",
    "    for j in range(ensemble):\n",
    "        trainlosses[j].append(trainloss[j])\n",
    "        acctracker[j].append(acc[j])\n",
    "        \n",
    "    pl.figure(1, figsize=(15,15))\n",
    "    for i in range(ensemble):\n",
    "        pl.subplot(ensemble,2,2*i+1)\n",
    "        pl.plot(trainlosses[i])\n",
    "        pl.ylim((0,3))\n",
    "        pl.title(\"Training Loss\")\n",
    "        pl.subplot(ensemble,2,2*i+2)\n",
    "        pl.plot(acctracker[i])\n",
    "        pl.ylim((0,1))\n",
    "        pl.title(\"Training/val Acc\")\n",
    "    pl.show()\n",
    "    print(\"Training loss is: \"+str(trainloss)+\n",
    "            \"\\nTraining accuracy is: \"+str(acc)+\"\\n\")\n",
    "    print(\"Approximately %.2f hours to completion\"%(  (time.time()-start)/(e+1)*(epochs-e)/3600  ))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Adjust learnrate\n",
    "    if e%epoch == 0:\n",
    "        [s.step() for s in scheduler]\n",
    "    \n",
    "print(\"Training complete: %.2f hours total\" % ((time.time()-start)/3600)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acclist = []\n",
    "pcacclist = []\n",
    "alldispacc = np.zeros(way)\n",
    "for r in range(n_trials):\n",
    "    # Accumulate foreground/background prototypes, if using\n",
    "    fbcentroids = (accumulateFB(models, repr_loader, way, network_width, ngiv, bsize)\n",
    "                   if include_masks else \n",
    "                   [None]*ensemble)\n",
    "    \n",
    "    # Accumulate category prototypes\n",
    "    centroids, counts = accumulate(models, repr_loader, expanders, \n",
    "                                   fbcentroids, way, d)\n",
    "    # Score the models\n",
    "    allacc, dispacc, perclassacc = score(k, centroids, fbcentroids, models, \n",
    "                                         query_loader, expanders, way)\n",
    "    # Record statistics\n",
    "    acclist = acclist+allacc\n",
    "    pcacclist = pcacclist+list(perclassacc)\n",
    "    alldispacc += dispacc\n",
    "\n",
    "# Aggregate collected statistics\n",
    "accs = sum(acclist)/n_trials/ensemble\n",
    "pcaccs = sum(pcacclist)/n_trials/ensemble\n",
    "alldispacc = alldispacc/n_trials\n",
    "confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)\n",
    "pcconfs = 1.96*np.sqrt(np.var(pcacclist)/n_trials/ensemble)\n",
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "torch.save([m.encode.cpu().state_dict() for m in models], savepath)\n",
    "\n",
    "# If using parametric localization, save the extra parameters\n",
    "if localizing and not fewshot_local:\n",
    "    torch.save([m.postprocess.centroids.cpu() for m in models], \n",
    "               savepath[:savepath.rfind('.')]+'_localizers'+savepath[savepath.rfind('.'):])\n",
    "print(\"Models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
