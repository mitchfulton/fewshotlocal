{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from helpful_files.networks import Network, predict\n",
    "from helpful_files.training import *\n",
    "from helpful_files.testing import score,OrderedSampler\n",
    "\n",
    "from KiTS_dataset import KiTS_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "# General settings\n",
    "datapath = './'                     # The location of your train, test, repr, and query folders. Make sure it ends in '/'!\n",
    "savepath = 'myModel.pth'            # Where should your trained model(s) be saved, and under what name?\n",
    "gpu = 0                             # What gpu do you wish to train on?\n",
    "workers = 6                         # Number of cpu worker processes to use for data loading\n",
    "epoch = 10                          # Number of passes over the dataset before the learning rate is cut\n",
    "ncuts = 5                           # Number of times to cut the learning rate before training completes\n",
    "verbosity = 5                       # How many batches in between status updates \n",
    "ensemble = 3                        # How many models to train in parallel\n",
    "k = 1                               # Evaluate top-k accuracy. Typically 1 or 5. \n",
    "torch.cuda.set_device(gpu)\n",
    "cudnn.benchmark = True\n",
    "trn_pct = 0.9                       #test/val split pct\n",
    "\n",
    "# Batch construction\n",
    "way = 4                            # Number of classes per batch during training\n",
    "trainshot = 2                       # Number of images per class used to form prototypes\n",
    "testshot = 1                       # Number of images per class used to make predictions\n",
    "\n",
    "num_classes = 6                    #for validation\n",
    "\n",
    "# Model construction\n",
    "folding = True                      # Use batch folding?\n",
    "covariance_pooling = True           # Use covariance pooling?\n",
    "localizing = True                   # Use localization?\n",
    "fewshot_local = False                # If you are using localization: few-shot, or parametric? Few-shot if True, param if False\n",
    "network_width = 32                  # Number of channels at every layer of the network\n",
    "\n",
    "# Data loading\n",
    "augmentation_flipping = True        # Horizontal flip data augmentation\n",
    "include_masks = (localizing         # Include or ignore the bounding box annotations?\n",
    "                 and fewshot_local)\n",
    "data_shape = (128,128,128)\n",
    "\n",
    "\n",
    "# Calculate embedding size based on model setup\n",
    "d = (network_width if not \n",
    "     covariance_pooling else\n",
    "     network_width**2)\n",
    "if localizing and not covariance_pooling:\n",
    "    d = network_width*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [19:57<00:00,  6.33s/it]\n",
      "100%|██████████| 21/21 [02:13<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Training Data \n",
    "#this step takes forever \n",
    "#sampler has to iterate through whole dataset and dataset is real big\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "d_boxes = torch.load('helpful_files/box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\"\"\"\n",
    "if folding:\n",
    "    # Batch folding has no reference/query distinction\n",
    "    shots = [trainshot+testshot]\n",
    "else:\n",
    "    # Standard setup\n",
    "    shots = [trainshot, testshot]\n",
    "if localizing and fewshot_local and not folding:\n",
    "    # Unfolded prototype localizers need another set of reference images to inform foreground/background predictions\n",
    "    shots = [trainshot, trainshot, testshot-trainshot]\n",
    "    \n",
    "\"\"\"\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    datapath+'train', \n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, augmentation_flipping, include_masks))\n",
    "\"\"\"\n",
    "trainset = KiTS_Set(data_shape)\n",
    "\n",
    "train_data,val_data = torch.utils.data.random_split(trainset,[round(trn_pct*len(trainset)), round((1-trn_pct)*len(trainset))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_sampler = ProtoSampler(train_data, way, shots),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data, \n",
    "    batch_sampler = OrderedSampler(val_data, shots[-1]), #batch size must be equal to shots\n",
    "    #batch_size = 3,\n",
    "    num_workers = workers,\n",
    "    #drop_last = True,\n",
    "    #shuffle = True,\n",
    "    pin_memory = True)\n",
    "\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140388 parameters in each neural net.\n",
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Make Models\n",
    "    \n",
    "models = [Network(network_width, folding, covariance_pooling, \n",
    "                  localizing, fewshot_local, shots).cuda() \n",
    "          for i in range(ensemble)]\n",
    "optimizer = [optim.Adam(m.parameters(), lr=.0001) for m in models]\n",
    "scheduler = [optim.lr_scheduler.LambdaLR(o, lambda x: 1/(2**x)) for o in optimizer]\n",
    "criterion = NLLLoss().cuda()\n",
    "\n",
    "\"\"\"\n",
    "expander = avgpool()\n",
    "if localizing:\n",
    "    if fewshot_local:\n",
    "        expander = fsCL if covariance_pooling else fsL\n",
    "    else:\n",
    "        expander = pCL() if covariance_pooling else pL()\n",
    "elif covariance_pooling:\n",
    "    expander = covapool\n",
    "expanders = [expander for _ in range(ensemble)]\n",
    "\n",
    "\n",
    "if localizing and not fewshot_local:\n",
    "    fbcentroids = torch.load(model[:model.rfind('.')]+'_localizers'+model[model.rfind('.'):])\n",
    "    for i in range(ensemble):\n",
    "        expanders[i].centroids.data = fbcentroids[i]\n",
    "        expanders[i].cuda()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "nweights = sum([i.numel() for i in list(models[0].parameters())])\n",
    "print(nweights,\"parameters in each neural net.\")\n",
    "print('Ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0])\n",
      "torch.Size([1, 1, 32, 2, 1, 1, 1]) torch.Size([3, 1, 1024])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0') tensor([0, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (192) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [1, 192].  Tensor sizes: [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60733bb1b41e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Score the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mallacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperclassacc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfbcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#print(allacc,dispacc,perclassacc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mitch/fewshotlocal/helpful_files/testing.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(k, centroids, bcentroids, models, loader, expanders, way)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;31m#print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (192) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [1, 192].  Tensor sizes: [1, 3]"
     ]
    }
   ],
   "source": [
    "# Do the Thing!\n",
    "\n",
    "start = time.time()\n",
    "trainlosses, acctracker = [[] for _ in range(ensemble)],[[] for _ in range(ensemble)]\n",
    "valacctracker = [[] for _ in range(ensemble)]\n",
    "\n",
    "epochs = ncuts*epoch\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    trainloss, acc = train(train_loader, models, optimizer, criterion, way, shots, verbosity)\n",
    "    \n",
    "    # Update the losses\n",
    "    display.clear_output(wait=True)\n",
    "    for j in range(ensemble):\n",
    "        trainlosses[j].append(trainloss[j])\n",
    "        acctracker[j].append(acc[j])\n",
    "    \n",
    "    \n",
    "    #now run the validation set\n",
    "    \n",
    "    #create necessary vars\n",
    "    expanders = [m.postprocess for m in models]\n",
    "    centroids = [e.centroids for e in expanders]\n",
    "    fbcentroids = [None]*ensemble\n",
    "    val_models = [m.encode for m in models]\n",
    "    acclist = []\n",
    "    pcacclist = []\n",
    "    alldispacc = np.zeros(num_classes)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Accumulate foreground/background prototypes, if using\n",
    "    fbcentroids = (accumulateFB(models, repr_loader, way, network_width, ngiv, bsize)\n",
    "                   if include_masks else \n",
    "                   [None]*ensemble)\n",
    "    # Accumulate category prototypes\n",
    "    centroids, counts = accumulate(models, repr_loader, expanders, \n",
    "                                   fbcentroids, way, d)\n",
    "    \n",
    "    # Score the models\n",
    "    allacc, dispacc, perclassacc = score(k, centroids, fbcentroids, models, \n",
    "                                         query_loader, expanders, way)\n",
    "    \"\"\"\n",
    "    # Score the models\n",
    "    allacc, dispacc, perclassacc = score(k, centroids, fbcentroids, val_models, val_loader, expanders, num_classes)\n",
    "    \n",
    "    #print(allacc,dispacc,perclassacc)\n",
    "    # Record val statistics\n",
    "    #acclist = acclist+allacc\n",
    "    #pcacclist = pcacclist+list(perclassacc)\n",
    "    #alldispacc += dispacc\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        nqueries = shots[-1]\n",
    "        #targ = torch.LongTensor([i//nqueries for i in range(nqueries*way)]).cuda()\n",
    "        val_acc = [0]*ensemble\n",
    "        for i, (img, dat, cd_score) in enumerate(val_loader):\n",
    "            #print(cd_score)\n",
    "            img = img.float().cuda()\n",
    "            cd_score = cd_score.cuda()\n",
    "            print(cd_score)\n",
    "            #print(img.size(),shots)\n",
    "            for j in range(ensemble):\n",
    "                out = predict(models[j].postprocess.centroids,models[j].postprocess(models[j].encode(img),_,__))\n",
    "                _,bins = torch.max(out,1)\n",
    "                #print(torch.sum(torch.eq(bins,cd_score)).item())\n",
    "                print(bins)\n",
    "                valacc_single = torch.sum(torch.eq(bins,cd_score)).item()/nqueries/way\n",
    "                print(valacc_single)\n",
    "                val_acc[j] += valacc_single\n",
    "        val_acc = [L/(i+1) for L in val_acc]\n",
    "    \n",
    "    \n",
    "    for j in range(ensemble):\n",
    "        valacctracker[j].append(val_acc[j])\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Aggregate collected val statistics\n",
    "    accs = sum(acclist)/ensemble\n",
    "    pcaccs = sum(pcacclist)/ensemble\n",
    "    alldispacc = alldispacc\n",
    "    confs = 1.96*np.sqrt(np.var(acclist)/ensemble)\n",
    "    pcconfs = 1.96*np.sqrt(np.var(pcacclist)/ensemble)\n",
    "\n",
    "    # Report val\n",
    "    print(\"Accuracies and 95% confidence intervals\")\n",
    "    print(\"Mean accuracy: \\t\\t%.2f \\t+/- %.2f\" % (accs*100, confs*100))\n",
    "    print(\"Per-class accuracy: \\t%.f \\t+/- %.2f\" % (pcaccs*100, pcconfs*100))\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #trn graphs\n",
    "    pl.figure(1, figsize=(15,15))\n",
    "    for i in range(ensemble):\n",
    "        pl.subplot(ensemble,2,2*i+1)\n",
    "        pl.plot(trainlosses[i])\n",
    "        pl.ylim((0,3))\n",
    "        pl.title(\"Training Loss\")\n",
    "        pl.subplot(ensemble,2,2*i+2)\n",
    "        pl.plot(acctracker[i])\n",
    "        pl.plot(valacctracker[i])\n",
    "        pl.ylim((0,1))\n",
    "        pl.title(\"Training/val Acc\")\n",
    "    pl.show()\n",
    "    print(\"Training loss is: \"+str(trainloss)+\n",
    "            \"\\nTraining accuracy is: \"+str(acc)+\"\\n\")\n",
    "    print(\"Validation accuracy is: \"+str(val_acc)+\"\\n\")\n",
    "    print(\"Approximately %.2f hours to completion\"%(  (time.time()-start)/(e+1)*(epochs-e)/3600  ))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Adjust learnrate\n",
    "    if e%epoch == 0:\n",
    "        [s.step() for s in scheduler]\n",
    "    \n",
    "print(\"Training complete: %.2f hours total\" % ((time.time()-start)/3600)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "torch.save([m.encode.cpu().state_dict() for m in models], savepath)\n",
    "\n",
    "# If using parametric localization, save the extra parameters\n",
    "if localizing and not fewshot_local:\n",
    "    torch.save([m.postprocess.centroids.cpu() for m in models], \n",
    "               savepath[:savepath.rfind('.')]+'_localizers'+savepath[savepath.rfind('.'):])\n",
    "print(\"Models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
